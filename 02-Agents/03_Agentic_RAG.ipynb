{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoCBQFSXJt1a"
      },
      "source": [
        "# Agentic RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
        "https://colab.research.google.com/github/martasumyk/ai_practice/blob/main/02-Agents/03_Agentic_RAG.ipynb\n",
        ")\n",
        "\n",
        "[![Open in GitHub](https://img.shields.io/badge/Open%20in-GitHub-black?logo=github)](\n",
        "https://github.com/martasumyk/ai_practice/blob/main/02-Agents/03_Agentic_RAG.ipynb\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz4Os-lyW2sP"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_-kOhiy-JoVW"
      },
      "outputs": [],
      "source": [
        "!pip install -q openai sentence-transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vo_U6zgXQIYh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from getpass import getpass\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from openai import OpenAI\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "from typing import List, Dict\n",
        "import re\n",
        "import json\n",
        "import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl0ZnPLkyMZU"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmMpDGFCyN0M"
      },
      "source": [
        "In this agentic RAG system, a user question first goes through a planner agent, which decides whether to answer directly or to retrieve information from external open-source knowledge bases. The system uses two vector-search knowledge bases:\n",
        "- AG News (real-world news)\n",
        "- SQuAD Wikipedia passages (factual knowledge)\n",
        "- plus an external calculator tool for math expressions.\n",
        "\n",
        "Based on the plan, a research agent retrieves relevant text chunks from one or both KBs and may call tools. The answer agent then generates a grounded response using only the retrieved context. A judge agent evaluates whether the answer is accurate and supported by the evidence; if not, it instructs a refinement step. Together, these agents create a flexible, multi-source, self-correcting pipeline that demonstrates how agentic RAG systems improve over standard RAG in adaptability, accuracy, and extensibility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdVxLYgpE9It"
      },
      "source": [
        "## OpenAI API key initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JA6xgne2OU6d",
        "outputId": "8f0f30b6-4919-43a9-d611-ab4c18ee8cdd"
      },
      "outputs": [],
      "source": [
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdmnUtP6pXZ2"
      },
      "source": [
        "## Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pjSbdHNDpZkT"
      },
      "outputs": [],
      "source": [
        "def simple_chunk(text: str, max_words: int = 120) -> List[str]:\n",
        "    \"\"\"Split a long text into chunks of at most max_words words.\"\"\"\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), max_words):\n",
        "        chunks.append(\" \".join(words[i:i + max_words]))\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUxtvlJSE_w-"
      },
      "source": [
        "## Data loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tRxfn-Vpwq-"
      },
      "source": [
        "### 1) AG News dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQum8B6-rKGb"
      },
      "source": [
        "AG is a collection of more than 1 million news articles. News articles have been gathered from more than 2000 news sources by ComeToMyHead in more than 1 year of activity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "568db2dd40784824a396da2099425d3e",
            "8d22eab5c9674776ae517de63c733683",
            "4fd17ce7a3bb40459338550ca73ea715",
            "1035dd0fda124f1eac684c6736c30111",
            "fa3a7eff9b9b4333bb25a46237acb7b3",
            "efb34902bb84413dbd54499e504cad27",
            "25f336579243473ba472d51f88f6de55",
            "df547f8779d9410cb6049f7b200a98bf",
            "e832101fbcb0476f80f294fe14b1f65a",
            "46f6960f769442748c357790cc2d9e26",
            "cac3ded640a549f785af10fcd135e75d",
            "a46f4338493640a0bda9a2caf5a6a63b",
            "6cd5e8f43c784ccd9f69a46ac933d92f",
            "112935dde31048cf993687cc0d89e09e",
            "7a2e2d8e58424514a3a1112f14b77973",
            "69361577a57f4b2aa42bd98de5d7f2eb",
            "54489563540448228a34d6ad2f09c587",
            "b86c9e9e2ca24721b213faf8995909d4",
            "55b0fc01098745d5beeb92f4bf74d1d2",
            "ddcb52ed405346c8923acda23b5f0a31",
            "dcb19af2da19493d8b08d34ef0d6d841",
            "4e9bc1293ab44fe185b2bb4d4cfdb7df",
            "af94972559674b90926b06da1dfa97af",
            "af6c360f54654c6d9a7bc1d7a8100e96",
            "6b36bc613db94070b26ae7f6b87906dc",
            "17256944eebe4081884d01d9e268a689",
            "90b1893be96148cc9a75e0cb8a10ca71",
            "1e75c4352de04001a951eb26cd7d59df",
            "03fad987029f41bd8234576cc69fe211",
            "249c60a3a2c94cf69cf6e93fe28c1a7e",
            "40f375ae3de842628d387b2454e90b53",
            "c6d42f22dc604a4aac59891d2681c4c1",
            "a444e03b26404bada2975e7e5431a432",
            "6d8647bc9d594bbc845c3980acb6586f",
            "26b4b3091a0b4b85a5a12f23b7b8aa6f",
            "6b799646d5bc4086a3460f95c56fad1d",
            "598f3fb0f65a4a008050ed06a3d3a85c",
            "e784365974df4477a40a4e44d03a7923",
            "623e5426b55c4760bb301192172f0f22",
            "6a89e1fc6463467c8e471b2ba9579d18",
            "8b5325ce09cc4a81a9d2a65b2e8f9e74",
            "3def1d1e39ef4a87bb51d76dc5d4dfcc",
            "ef50edbd77654be9911a6a6902124f8e",
            "aeb9081999a840d491c726c2a4a763ac"
          ]
        },
        "id": "HMmWWKQep9OW",
        "outputId": "2be48a00-a034-4a8e-c723-2ca232a3b297"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "568db2dd40784824a396da2099425d3e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a46f4338493640a0bda9a2caf5a6a63b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af94972559674b90926b06da1dfa97af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d8647bc9d594bbc845c3980acb6586f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "raw_news = load_dataset(\"ag_news\", split=\"train[:200]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "B7UpdARMpzIi"
      },
      "outputs": [],
      "source": [
        "news_docs: List[Dict] = []\n",
        "\n",
        "for i, row in enumerate(raw_news):\n",
        "    chunks = simple_chunk(row[\"text\"], max_words=100)\n",
        "    for j, ch in enumerate(chunks):\n",
        "        news_docs.append(\n",
        "            {\n",
        "                \"kb\": \"news\",        # which knowledge base it belongs to\n",
        "                \"doc_id\": i,         # original article id\n",
        "                \"chunk_id\": j,       # chunk index inside article\n",
        "                \"label\": int(row[\"label\"]),\n",
        "                \"text\": ch,          # the chunk text\n",
        "            }\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NwzQbjqqpIg",
        "outputId": "ee1f1afa-35cc-4296-bb38-3c563564d371"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'chunk_id': 0,\n",
            " 'doc_id': 0,\n",
            " 'kb': 'news',\n",
            " 'label': 2,\n",
            " 'text': 'Wall St. Bears Claw Back Into the Black (Reuters) Reuters - '\n",
            "         \"Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are \"\n",
            "         'seeing green again.'}\n",
            "\n",
            "================================================================================\n",
            "\n",
            "{'chunk_id': 0,\n",
            " 'doc_id': 1,\n",
            " 'kb': 'news',\n",
            " 'label': 2,\n",
            " 'text': 'Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - '\n",
            "         'Private investment firm Carlyle Group,\\\\which has a reputation for '\n",
            "         'making well-timed and occasionally\\\\controversial plays in the '\n",
            "         'defense industry, has quietly placed\\\\its bets on another part of '\n",
            "         'the market.'}\n",
            "\n",
            "================================================================================\n",
            "\n",
            "{'chunk_id': 0,\n",
            " 'doc_id': 2,\n",
            " 'kb': 'news',\n",
            " 'label': 2,\n",
            " 'text': \"Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring \"\n",
            "         'crude prices plus worries\\\\about the economy and the outlook for '\n",
            "         'earnings are expected to\\\\hang over the stock market next week '\n",
            "         'during the depth of the\\\\summer doldrums.'}\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for example in news_docs[:3]:\n",
        "    pprint.pprint(example)\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIHL6Jlzr-8d"
      },
      "source": [
        "### 2) SQuAD (open-source Wikipedia QA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "afb60d324e414f0e990618be95f3b990",
            "0b32c4a216a8472f86a82e06f234890b",
            "1c6a7195222e41e99336d301271c5f08",
            "19512282f2a14818a5dd0afab0aa2c4e",
            "fb390e8360e54ebebaba4bfc934278b9",
            "640a3e0c12554731ae332ff1ad526cce",
            "603e0c433cd5421f8607a5fb053af467",
            "d378595e886a48089451deee37a50e71",
            "84d98f9f17f4465ebf56ee3ca3f3ad62",
            "d0655e306b74428588be2284cc84ec58",
            "d7bd440ed79e4042922d1b6d3dfb68f4",
            "d471babff0754ddfba1bf4db0da184f1",
            "ca843f3712a644f6aac2459c0d9bbc73",
            "bb40c26f3d4d4cb58baacaedceddf141",
            "eeef96977f4f42bea73464f5e5c49528",
            "30e8cc37e0c845deaaab346cb8d8367a",
            "eef33f89cc8748c1b906c8cc8c28c8f0",
            "bf0eb0f9a89c4fe8976dba918ca9a990",
            "00f2c28833af41b19c3fc766b8f5fb6a",
            "99d3a75094214b5998e676b197897129",
            "aa521e60a3a147358f3fdc79155d4399",
            "0e34c2d428e447da9650d17986718696",
            "d6d81331a06e4bb39cb64eaea122570d",
            "7e746e24af394efa854264f041f90620",
            "ed1ce3fa88ce4ed798c24c93ba551ad1",
            "82cd59b830d4477698d9246611e4adce",
            "e45ff5f3a6274ff299058dce951dc322",
            "8d6d9a3aabc8445dac4ce561924a4b8f",
            "f72d7c77eea340c19efe82169b741897",
            "0616d2c478b94e6c81cb1dcd1026233f",
            "9012edee6af4407289bdb95cab2f2be2",
            "9f758fb8ae40432bad78d91488b1c7fb",
            "8edb3590c6b74680872b8062351be45d",
            "c44199f8a8e743fa8a3ab2a7b2f726d5",
            "bee4f29bbc90479b9b46473b4a14ad82",
            "c1bb3ebea7c54b19becc7f7ff736bffd",
            "0fb0e3e6c66f429086b03b591171aa0b",
            "482c23ce17a5488a84eedb7555fef242",
            "dde201f7ba4e4496b5a03b1e25a21b6e",
            "8afea9721c59404893c74dd695f09c68",
            "d3ef696212664b799192bda48b17cded",
            "e753b46acf7e4d29bf69a551a7be1e38",
            "a09005ebb66843a2b4be82225f5ff2d5",
            "cc90b76f12f7423fb234ab8bb7a49a14",
            "dd81123ffb634d6a870463af9652be61",
            "3bc77c3e47584c7285a7b98b84caa7db",
            "c7882b0b896a4ced88c84adf3cb5cd3b",
            "243cc268a4aa47eea3f703feb5b021e6",
            "e4440f08d36342738f5d7531f439d7cf",
            "82585dbf4a4b473dbe714a1e0a7390c8",
            "0edd032ce7234c1ab273b84d5c6bd534",
            "fb72f32da871495d805097fe2d4f8242",
            "9431bb0502834c95aa64635630a39603",
            "1a194e59fb2a44ceb76fcd714ef6aaf1",
            "50d61a8f91a24c06b872b43239fa5d87"
          ]
        },
        "id": "QGEU1cdWqx_g",
        "outputId": "7796b7de-9947-4ac0-f283-921e6942d9cf"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "afb60d324e414f0e990618be95f3b990",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d471babff0754ddfba1bf4db0da184f1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "plain_text/train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d6d81331a06e4bb39cb64eaea122570d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "plain_text/validation-00000-of-00001.par(â€¦):   0%|          | 0.00/1.82M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c44199f8a8e743fa8a3ab2a7b2f726d5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd81123ffb634d6a870463af9652be61",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "raw_squad = load_dataset(\"squad\", split=\"train[:200]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QHp9kHnjsEjA"
      },
      "outputs": [],
      "source": [
        "squad_docs: List[Dict] = []\n",
        "\n",
        "for i, row in enumerate(raw_squad):\n",
        "    context = row[\"context\"]\n",
        "    question = row[\"question\"]\n",
        "\n",
        "    base_text = f\"Q: {question}\\nCONTEXT: {context}\" # using both questions and context\n",
        "\n",
        "    chunks = simple_chunk(base_text, max_words=100)\n",
        "    for j, ch in enumerate(chunks):\n",
        "        squad_docs.append(\n",
        "            {\n",
        "                \"kb\": \"squad\",      # name of second KB\n",
        "                \"doc_id\": i,\n",
        "                \"chunk_id\": j,\n",
        "                \"label\": None,\n",
        "                \"text\": ch,\n",
        "            }\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COfMZoaEsMRf",
        "outputId": "8c4db0aa-074f-43c8-b500-0ae00f554a86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SQuAD: 434 chunks from 200 examples\n"
          ]
        }
      ],
      "source": [
        "print(f\"SQuAD: {len(squad_docs)} chunks from {len(raw_squad)} examples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMP0rqjcsOy0",
        "outputId": "d04746a4-1dab-4513-fcf5-bd9c0993cd0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'chunk_id': 0,\n",
            " 'doc_id': 0,\n",
            " 'kb': 'squad',\n",
            " 'label': None,\n",
            " 'text': 'Q: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes '\n",
            "         'France? CONTEXT: Architecturally, the school has a Catholic '\n",
            "         \"character. Atop the Main Building's gold dome is a golden statue of \"\n",
            "         'the Virgin Mary. Immediately in front of the Main Building and '\n",
            "         'facing it, is a copper statue of Christ with arms upraised with the '\n",
            "         'legend \"Venite Ad Me Omnes\". Next to the Main Building is the '\n",
            "         'Basilica of the Sacred Heart. Immediately behind the basilica is the '\n",
            "         'Grotto, a Marian place of prayer and reflection. It is a replica of '\n",
            "         'the grotto at Lourdes, France where the'}\n",
            "\n",
            "================================================================================\n",
            "\n",
            "{'chunk_id': 1,\n",
            " 'doc_id': 0,\n",
            " 'kb': 'squad',\n",
            " 'label': None,\n",
            " 'text': 'Virgin Mary reputedly appeared to Saint Bernadette Soubirous in '\n",
            "         '1858. At the end of the main drive (and in a direct line that '\n",
            "         'connects through 3 statues and the Gold Dome), is a simple, modern '\n",
            "         'stone statue of Mary.'}\n",
            "\n",
            "================================================================================\n",
            "\n",
            "{'chunk_id': 0,\n",
            " 'doc_id': 1,\n",
            " 'kb': 'squad',\n",
            " 'label': None,\n",
            " 'text': 'Q: What is in front of the Notre Dame Main Building? CONTEXT: '\n",
            "         'Architecturally, the school has a Catholic character. Atop the Main '\n",
            "         \"Building's gold dome is a golden statue of the Virgin Mary. \"\n",
            "         'Immediately in front of the Main Building and facing it, is a copper '\n",
            "         'statue of Christ with arms upraised with the legend \"Venite Ad Me '\n",
            "         'Omnes\". Next to the Main Building is the Basilica of the Sacred '\n",
            "         'Heart. Immediately behind the basilica is the Grotto, a Marian place '\n",
            "         'of prayer and reflection. It is a replica of the grotto at Lourdes, '\n",
            "         'France where the Virgin Mary reputedly'}\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for example in squad_docs[:3]:\n",
        "    pprint.pprint(example)\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7M54OrisUG6"
      },
      "source": [
        "## Build embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ns-HMIFSsoRg"
      },
      "source": [
        "Model for embeddings: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "801642986afb47108ebea55b635afab3",
            "60bf409f4a2b4ab596efae7046fba1ec",
            "77a68f3374044248b0de159e6b12fcc4",
            "ea15294e031d46c38a092b50cef192db",
            "09917af3ad6c4c0d9823e7ea53c5d4cc",
            "01591f3d528645a2aaed5eee9c713fa8",
            "c0072fff40de4e7993c57d925f463fa7",
            "b514a2ced25944a081f3aa38205250f4",
            "52150930c32247d78bd1b78c0402ac2e",
            "f00e4e728ea046bcb2446b3e73f9f26f",
            "f78f1b6690ab4f4e8e0da3de47d39e19",
            "3c9f610fd9514e20b17bf9f24ac213a4",
            "bd11efd5bcdd46e6b15445eb216aef15",
            "389701c9f445407eaac04ea56c2a535f",
            "cffc896b15f649149fe4d60d37215626",
            "8e4c1cba20ce45aaada641a935dd74a6",
            "7941ddaaa4c449bdbe48bc0a17499c8e",
            "c77046fe961b47208683658897db9c92",
            "771dd74d496144cf98522913ef6466a3",
            "cd373c14c45c4300835d857704d927e8",
            "bc7a7e32b5cd4e7892fcd07f415ed8ed",
            "2816dc45faa54d4692b4cab7d10ccf92",
            "84057e9ae2db44ba9dd63109857d94e9",
            "326ff25b60074194b14868b94211f5a8",
            "b1711489a9b247d59ac8ea0eb8778675",
            "8fe5470366474f238b0fcf9b0cfdca2a",
            "e594a0222c084e309412033a14340afc",
            "454ec3cf2f574dfd9acd93e0d3dee5f4",
            "2bce6b7c67274fc1913493e308e4f4b0",
            "5dafd125157b45dfbaf2a07bd52c4b15",
            "f140b852e985441cba86991b2446a9a4",
            "75bbfb3ff7e64d1988929261538e2e7e",
            "60be7c864ef949098e4a60e427be97ad",
            "9578be0b07a544a4ac92a1b8b69063a0",
            "279318cf67e94a07a02ffef035f2cbca",
            "de9931f1718d4fd6aa99ac4f31ee81d2",
            "6601d3839d2e40baa31981ebd9529dc6",
            "fe06ea2401e74a8e8782f51c14f0faa5",
            "bef1270839d8481d84ead4de6d003d7a",
            "8adbd352b00b4aecb87546da6e55a276",
            "ec3d9fc6774e4a649ce3a71ed73a97d2",
            "96a14e6053c4439cbd4a9c7ea379d8b6",
            "1833677fba9a4d5594d881b17ed20a5b",
            "8f7f25714d054ed28bd36ed96d49fef8",
            "d4e303dbe5834e058fba0cecd95d95ec",
            "ef9eae85ef2c4f36b1cd5292dfb716fd",
            "0badbb0431cb44f39d0452848cdbe631",
            "b348ab0b405e4779accd428b0c92e158",
            "39445dca045f4c7aadf3a794e25bbff4",
            "350e42525a894838a9f317ce0b5b6800",
            "c0499439cf154c29be46f8725faf4f02",
            "2487090d9c8242df8b322b09ebb0a890",
            "d13fa28aca7549609f85374d56df2225",
            "cb561daad8a34191892e688a9903f6ff",
            "9a3c338117e44d16a3a759c031695a74",
            "17ed74991fdc4a9d815f98194c2d6a11",
            "cce88c0091f44557a4c9f92406b1b9d2",
            "bbbc49cbc8f64f928010d3e399bdac72",
            "113be1d1c5334e3296af358d95ce1ef4",
            "4fbd3397ef2343b6abafbe3709819b75",
            "d8f550ff7cab4fb0a7f71c87222200e6",
            "45ed78a616eb4e079c4bd01e76fc085c",
            "9caecd95ed604ab9a2bba0640a975495",
            "26a11b5a501e46ea9944a6d0c1d73c36",
            "f6a12edaec2245488a29a184e8a219f6",
            "f8e9d837951146318f317d4096355912",
            "6a4616a15bd24379bf335594bd4ddc82",
            "cec9d5f97d9d4f5e84fea65071378f49",
            "465e822ebf8d4ae9b37c39dda65b274a",
            "ff88260f4064427b823b90e823a67a9f",
            "09a746ae3ebf4ce3957778ff8f5672b5",
            "bed0f2292f6a420ba8b9adc7990029c8",
            "83fcff30d09e47588a06228c07c73165",
            "6d903d25444f45e6896de2d7144f457b",
            "8681254a4cc54cf7b2ec5a6d776f6081",
            "20ca821d7bc44136b3df906c889ad2bc",
            "07708ca934344eeaa186837273c7f2a3",
            "256567e312e8498996985e5237bcb43c",
            "4b57504d3f0d47ee989c89b20bceaaed",
            "92099c0414684217a1c8b42f8446353c",
            "dcf859788e504edf86ed128e81d77380",
            "270c264c8d5d423d8ea3452f0eae0d4f",
            "bb81edea92be4ce09b4008ec694e25dc",
            "515e38803cd34589a65eb8e4f562b847",
            "d8ef4765a9ea4ba18c3fd681d09870b2",
            "b6e9d20e662a43a99aafa7969cd37972",
            "3eee0647052147a8bf1b2fdbde0386ac",
            "4623436fb41c4e3ab1caf4decae47604",
            "7bf091d26e1a44d094e8488a5c8aa15c",
            "479dba1f8636488baf2b9bc2cc38f334",
            "9735f1c0bc3347fba6e80eb6f1510d8c",
            "560db925392645d791e5d2ddc52f7a8c",
            "8ddd0abe11bf4fe2b4f91ec0ccca853b",
            "7f260dbab898449b91444a02d5135865",
            "1a372d9e4e64471e99ba02ed504f53ae",
            "5503a62542bf405faa76ff2f652dacb3",
            "436a78015d8a4fd6873cabb19e397c6c",
            "adbf2c5f83bb4f75a1caefac3ffb816f",
            "f42f8af896474fdd954dcf0ed868572a",
            "b0bda38411f948ee8dd7cdda0ace0213",
            "71c4164aea95424ab75c7ffc6a35b98a",
            "0956551490984d90a5cddf550506edfe",
            "9c07d957127642e9a78aed2e1ac721f8",
            "cae36d78c3db4ed1b2f3ecfa969c12ae",
            "95c1296fba1445f0b3d30ca66a721bbe",
            "41c403db475d41da80d2c39b9b505f2d",
            "0b9c66006f0845e59232b0f8c1a6601f",
            "c411c1fad3f7424089cd4ea90f90130a",
            "0f99d8ca9fe6469e9c47280e918da5b8",
            "cf8e3dd79aab4e15a82937b316e1446e",
            "2a64c5c8509b471f9e128b2126c6aacd",
            "7e72157b0bb748028b9734166aa2f5d5",
            "9244432a88ba48c5bf78c5c86a1c9491",
            "5b3b701138684e6e9bf67b50b2fc4421",
            "95773fdc94704f09bc032637f9f5385c",
            "f461f0c35f9446d18d544c2eb8be3c38",
            "3226467443e1403cb10a0727a0ac4f1c",
            "f79e888c4d2f4218be8570b9cb480de3",
            "79d2357b8a75402b9e1fa317bf5ecdec",
            "653cf91da8244be7b168db2815edf0a3",
            "9934015e768240d59c01ab276437aef9"
          ]
        },
        "id": "_aI1WRmgsj2p",
        "outputId": "bbcaabe1-5694-4b88-a49a-a43bb5cbcc5c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "801642986afb47108ebea55b635afab3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c9f610fd9514e20b17bf9f24ac213a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "84057e9ae2db44ba9dd63109857d94e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9578be0b07a544a4ac92a1b8b69063a0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d4e303dbe5834e058fba0cecd95d95ec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "17ed74991fdc4a9d815f98194c2d6a11",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6a4616a15bd24379bf335594bd4ddc82",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "256567e312e8498996985e5237bcb43c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7bf091d26e1a44d094e8488a5c8aa15c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b0bda38411f948ee8dd7cdda0ace0213",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a64c5c8509b471f9e128b2126c6aacd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embedder = SentenceTransformer(EMBED_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "r_pQZ8sRsYz6"
      },
      "outputs": [],
      "source": [
        "def build_embeddings(docs: List[Dict]):\n",
        "    texts = [d[\"text\"] for d in docs]\n",
        "    embs = embedder.encode(\n",
        "        texts,\n",
        "        convert_to_numpy=True,\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "    norms = np.linalg.norm(embs, axis=1, keepdims=True) + 1e-12\n",
        "    embs = embs / norms\n",
        "    return embs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "bbb62d30ffd7416f845da52bee9a921c",
            "95c4b3d404064d7c9ce376c40703b5fc",
            "a4095b1cca6340af967a1bd4695d5191",
            "ddc09fcadfa94fd99469d0045a372c1e",
            "d85a0ff2079c42828f16be4d62a23d1a",
            "257ac70f92154b6a8670592019d1c992",
            "96975401e8fe437db4c8659e33e8cd19",
            "f6e3554a725941119fc1acfd88b934e7",
            "2e7e70aee0fb44dfbb69a0cb6a6b8ba8",
            "dd84dbe4f9c04cf6bd5d46db8786d9ad",
            "0743300f54ce4f9abe0a2c99adb1996d",
            "09142faaf7344783b6f332241fc08a23",
            "ff2b835652524e77b3e1c4f9079e2c99",
            "a0f4ef0538da44568a8a67dd79d7d3c8",
            "afee3dfb13bb41a4ac978d07e8c9db29",
            "b16a9422f32449799be4cf6a10e58245",
            "28ac44d27bab4f17acc0b74d2d1bf723",
            "24c7b32d3c094259b657bbdf9e436268",
            "486854bf092c41c08f37c502779dd2e6",
            "88cb20d94c9d4445a054888e20027a29",
            "bae76615a0724686bc67cab2c8067288",
            "cfd93c9179284e2bb3c23a068bdffef7"
          ]
        },
        "id": "sUT8I2ZYsvGw",
        "outputId": "83321a13-7c43-4165-f7b7-a954de67885e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bbb62d30ffd7416f845da52bee9a921c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09142faaf7344783b6f332241fc08a23",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/14 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "news_embeddings = build_embeddings(news_docs)\n",
        "squad_embeddings = build_embeddings(squad_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZnTyHQ7sxTI"
      },
      "source": [
        "Now, our knowledge base consists of pairs of texts and their corresponding embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lf5twZZQsVzy",
        "outputId": "64f7bea3-821e-49fd-cad4-88ea2d2d6b47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "news -> 215 docs/chunks\n",
            "squad -> 434 docs/chunks\n"
          ]
        }
      ],
      "source": [
        "KB = {\n",
        "    \"news\": {\n",
        "        \"docs\": news_docs,\n",
        "        \"embeddings\": news_embeddings,\n",
        "    },\n",
        "    \"squad\": {\n",
        "        \"docs\": squad_docs,\n",
        "        \"embeddings\": squad_embeddings,\n",
        "    },\n",
        "}\n",
        "\n",
        "for name, kb in KB.items():\n",
        "    print(name, \"->\", len(kb[\"docs\"]), \"docs/chunks\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PL2tPODitF65"
      },
      "source": [
        "## Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFj0pPYitMyg"
      },
      "source": [
        "Given a `kb_name` and a query, find the top-k most similar chunks with cosine similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Pc5SlMSutG7x"
      },
      "outputs": [],
      "source": [
        "def retrieve_from_kb(kb_name: str, query: str, k: int = 5) -> List[Dict]:\n",
        "    \"\"\"Vector search in the chosen knowledge base.\"\"\"\n",
        "    kb = KB[kb_name]\n",
        "    docs = kb[\"docs\"]\n",
        "    embs = kb[\"embeddings\"]\n",
        "\n",
        "    q_emb = embedder.encode([query], convert_to_numpy=True)\n",
        "    q_emb = q_emb / (np.linalg.norm(q_emb, axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "    # cosine similarity via dot product (since vectors are normalized)\n",
        "    scores = np.dot(embs, q_emb[0])\n",
        "    top_idx = np.argsort(scores)[::-1][:k]\n",
        "\n",
        "    results = []\n",
        "    for idx in top_idx:\n",
        "        d = docs[idx]\n",
        "        results.append(\n",
        "            {\n",
        "                \"score\": float(scores[idx]),\n",
        "                \"kb\": kb_name,\n",
        "                \"doc_id\": d[\"doc_id\"],\n",
        "                \"chunk_id\": d[\"chunk_id\"],\n",
        "                \"text\": d[\"text\"],\n",
        "            }\n",
        "        )\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23q8YOy-tqu5",
        "outputId": "ecd5c694-bf33-4221-c039-fb4b123bbcbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "News retrieval example:\n",
            "news 0.6110310554504395 -> Oil and Economy Cloud Stocks' Outlook NEW YORK (Reuters) - Soaring crude prices plus worries about the economy and the o ...\n",
            "\n",
            "news 0.5996482968330383 -> Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the ou ...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"News retrieval example:\")\n",
        "for r in retrieve_from_kb(\"news\", \"stock markets and oil prices\", k=3):\n",
        "    print(r[\"kb\"], r[\"score\"], \"->\", r[\"text\"][:120], \"...\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVrQxGEpttA_",
        "outputId": "9cb08af3-bdf8-4aae-b2da-2fbc517e08e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FAQ retrieval example:\n",
            "squad 0.43769073486328125 -> located in Beijing, Chicago, Dublin, Jerusalem and Rome. ...\n",
            "\n",
            "squad 0.43769073486328125 -> located in Beijing, Chicago, Dublin, Jerusalem and Rome. ...\n",
            "\n",
            "squad 0.40566298365592957 -> Q: How large in square feet is the LaFortune Center at Notre Dame? CONTEXT: A Science Hall was built in 1883 under the d ...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"FAQ retrieval example:\")\n",
        "for r in retrieve_from_kb(\"squad\", \"Where is the Eiffel Tower located?\", k=3):\n",
        "    print(r[\"kb\"], r[\"score\"], \"->\", r[\"text\"][:120], \"...\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tjm0j3DatgwE"
      },
      "source": [
        "Small tool for calculations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "nfIm0zqetJme"
      },
      "outputs": [],
      "source": [
        "def calculator_tool(expression: str) -> str:\n",
        "    \"\"\"\n",
        "    A tiny math tool.\n",
        "    Only allows digits, +, -, *, /, parentheses, and spaces.\n",
        "    \"\"\"\n",
        "    if not re.fullmatch(r\"[0-9+\\-*/().\\s]+\", expression):\n",
        "        return \"Calculator error: unsupported characters.\"\n",
        "\n",
        "    try:\n",
        "        result = eval(expression, {\"__builtins__\": {}})\n",
        "        return f\"{expression} = {result}\"\n",
        "    except Exception as e:\n",
        "        return f\"Calculator error: {e}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNhUtPSHteBF"
      },
      "source": [
        "Quick check of toll usage:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-0VCUM7toCO",
        "outputId": "483bf30f-b4b2-4b79-bb6f-feb94f289b0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculator test: 3*(5+2) - 4 = 17\n"
          ]
        }
      ],
      "source": [
        "print(\"Calculator test:\", calculator_tool(\"3*(5+2) - 4\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDau4VX4u1x4"
      },
      "source": [
        "## LLM helper + Planner agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBdAZjDZu7gb"
      },
      "source": [
        "### LLM helper function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wysrw_sTu4J0"
      },
      "source": [
        "A small wrapper around the OpenAI Chat Completions API, to reduce repetition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "VBtwufXwu2HL"
      },
      "outputs": [],
      "source": [
        "def call_llm(system: str, user: str, model: str = \"gpt-4o-mini\") -> str:\n",
        "    resp = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system},\n",
        "            {\"role\": \"user\", \"content\": user},\n",
        "        ],\n",
        "    )\n",
        "    return resp.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74R2wQheu9-v"
      },
      "source": [
        "### Planner agent: decide mode, sources, and tool use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVJLoc_KvBP1"
      },
      "source": [
        "The planner reads the question and outputs JSON with:\n",
        "\n",
        "- `mode`: \"DIRECT\" or \"RAG\"\n",
        "\n",
        "- `sources`: which knowledge base to use\n",
        "\n",
        "- `use_calculator`: whether to call the math tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "81fFL38_u5mY"
      },
      "outputs": [],
      "source": [
        "def plan_agent(question: str) -> dict:\n",
        "    system = \"You are a routing agent that outputs ONLY valid JSON, no explanation.\"\n",
        "    user = f\"\"\"\n",
        "You help decide how to answer user questions using tools.\n",
        "\n",
        "TOOLS AND SOURCES:\n",
        "- \"news\": vector search over news articles (AG News dataset).\n",
        "- \"squad\": vector search over Wikipedia-style QA contexts (SQuAD dataset).\n",
        "- \"calculator\": a math tool for expressions like \"3*(5+2) - 4\".\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. If the question is about current or past events, business, finance, politics, or real-world news,\n",
        "   set mode to \"RAG\" and include \"news\" in sources.\n",
        "\n",
        "2. If the question looks like a factual 'what/when/where/why/how' question that a Wikipedia article\n",
        "   might answer, set mode to \"RAG\" and include \"squad\" in sources.\n",
        "\n",
        "3. If the question contains a math expression that can be evaluated, set \"use_calculator\": true.\n",
        "\n",
        "4. If the question is simple chit-chat or general knowledge, set mode to \"DIRECT\"\n",
        "   and use an empty sources list.\n",
        "\n",
        "Return ONLY valid JSON with keys:\n",
        "- \"mode\": \"DIRECT\" or \"RAG\"\n",
        "- \"sources\": an array from [\"news\", \"squad\"]\n",
        "- \"use_calculator\": true or false\n",
        "\n",
        "Example output:\n",
        "{{\"mode\": \"RAG\", \"sources\": [\"news\", \"squad\"], \"use_calculator\": false}}\n",
        "\n",
        "Now decide for this question:\n",
        "\"{question}\"\n",
        "\"\"\".strip()\n",
        "\n",
        "    raw = call_llm(system, user)\n",
        "\n",
        "    try:\n",
        "        plan = json.loads(raw)\n",
        "    except json.JSONDecodeError:\n",
        "        m = re.search(r\"\\{.*\\}\", raw, re.DOTALL)\n",
        "        if m:\n",
        "            plan = json.loads(m.group(0))\n",
        "        else:\n",
        "            plan = {\"mode\": \"DIRECT\", \"sources\": [], \"use_calculator\": False}\n",
        "\n",
        "    return plan\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIlcGrXFvbmJ"
      },
      "source": [
        "Planner quick test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FO3LF3oFvRj-",
        "outputId": "95c30db9-c981-4f7c-f2d3-5fae1702da12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QUESTION: Explain the idea of Hamiltonian oscillations.\n",
            "{'mode': 'RAG', 'sources': ['squad'], 'use_calculator': False}\n",
            "------------------------------------------------------------\n",
            "QUESTION: What happened in the stock market and oil prices recently?\n",
            "{'mode': 'RAG', 'sources': ['news'], 'use_calculator': False}\n",
            "------------------------------------------------------------\n",
            "QUESTION: Compute 3*(7+5).\n",
            "{'mode': 'DIRECT', 'sources': [], 'use_calculator': True}\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for q in [\n",
        "    \"Explain the idea of Hamiltonian oscillations.\",\n",
        "    \"What happened in the stock market and oil prices recently?\",\n",
        "    \"Compute 3*(7+5).\",\n",
        "]:\n",
        "    print(\"QUESTION:\", q)\n",
        "    print(plan_agent(q))\n",
        "    print(\"-\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0or7BzjwOTU"
      },
      "source": [
        "Works nice!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EC0QeRIwSpM"
      },
      "source": [
        "### Convert research output into a text context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOy71LvQwUko"
      },
      "source": [
        "Format the retrieved chunks and tool outputs into a single text block for the answer agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "0yeAzFxQwOiB"
      },
      "outputs": [],
      "source": [
        "def build_context_snippet(research_output: dict) -> str:\n",
        "    \"\"\"Pretty-print context from knowledge base chunks and tool outputs.\"\"\"\n",
        "    parts = []\n",
        "\n",
        "    for i, ch in enumerate(research_output[\"chunks\"]):\n",
        "        parts.append(\n",
        "            f\"[KB {ch['kb']} | doc {ch['doc_id']} | chunk {ch['chunk_id']} | score={ch['score']:.3f}]\\n\"\n",
        "            + ch[\"text\"]\n",
        "        )\n",
        "\n",
        "    for t in research_output[\"tools\"]:\n",
        "        parts.append(\n",
        "            f\"[TOOL {t['tool']}]\\ninput: {t['input']}\\noutput: {t['output']}\"\n",
        "        )\n",
        "\n",
        "    return \"\\n\\n\".join(parts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHvUNeVlxdn2"
      },
      "source": [
        "## Research agent: to execute plan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "60aI-THQxf8C"
      },
      "outputs": [],
      "source": [
        "def research_agent(question: str, plan: dict, k_per_source: int = 4) -> dict:\n",
        "    context_chunks = []\n",
        "\n",
        "    # 1) vector retrieval from each selected knowledge base\n",
        "    for source in plan.get(\"sources\", []):\n",
        "        if source in KB:\n",
        "            hits = retrieve_from_kb(source, question, k=k_per_source)\n",
        "            context_chunks.extend(hits)\n",
        "\n",
        "    # 2) optional calculator\n",
        "    tool_outputs = []\n",
        "    if plan.get(\"use_calculator\"):\n",
        "        tool_outputs.append(\n",
        "            {\n",
        "                \"tool\": \"calculator\",\n",
        "                \"input\": question,\n",
        "                \"output\": calculator_tool(question),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return {\n",
        "        \"question\": question,\n",
        "        \"plan\": plan,\n",
        "        \"chunks\": context_chunks,\n",
        "        \"tools\": tool_outputs,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "OZL8l06-xkWi"
      },
      "outputs": [],
      "source": [
        "def build_context_snippet(research_output: dict) -> str:\n",
        "    \"\"\"Pretty-print context for the answer agent.\"\"\"\n",
        "    parts = []\n",
        "\n",
        "    for i, ch in enumerate(research_output[\"chunks\"]):\n",
        "        parts.append(\n",
        "            f\"[KB {ch['kb']} | doc {ch['doc_id']} | chunk {ch['chunk_id']} | score={ch['score']:.3f}]\\n\"\n",
        "            + ch[\"text\"]\n",
        "        )\n",
        "\n",
        "    for t in research_output[\"tools\"]:\n",
        "        parts.append(f\"[TOOL {t['tool']}]\\ninput: {t['input']}\\noutput: {t['output']}\")\n",
        "\n",
        "    return \"\\n\\n\".join(parts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feFQXElOwYB9"
      },
      "source": [
        "## Answer agent + Judge agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1N59sbMwyYc"
      },
      "source": [
        "### Answer agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaJ7UykuwbWd"
      },
      "source": [
        "If `mode = DIRECT`, answer from general knowledge.\n",
        "If `mode = RAG`, answer using the context (knowledge base + tools)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Zy8wWfNWwVvo"
      },
      "outputs": [],
      "source": [
        "def answer_agent(research_output: dict) -> str:\n",
        "    question = research_output[\"question\"]\n",
        "    plan = research_output[\"plan\"]\n",
        "    mode = plan.get(\"mode\", \"DIRECT\")\n",
        "\n",
        "    # DIRECT mode just LLM\n",
        "    if mode == \"DIRECT\":\n",
        "        system = \"You are a helpful assistant. Answer concisely and clearly.\"\n",
        "        user = f\"Question: {question}\"\n",
        "        return call_llm(system, user)\n",
        "\n",
        "    # RAG mode\n",
        "    context = build_context_snippet(research_output)\n",
        "    system = \"You are a careful RAG assistant. Use only the given context when possible.\"\n",
        "    user = f\"\"\"\n",
        "You are given:\n",
        "\n",
        "QUESTION:\n",
        "{question}\n",
        "\n",
        "CONTEXT (from multiple knowledge bases and tools):\n",
        "{context}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "- Use the context to answer in 3â€“6 sentences.\n",
        "- If something is not supported by the context, say you are not sure.\n",
        "- At the end, list which KBs or tools you relied on, e.g. \"Sources: news, agentic_faq, calculator\".\n",
        "\"\"\".strip()\n",
        "\n",
        "    return call_llm(system, user)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdx_dpy0w8vr"
      },
      "source": [
        "### Judge agent (self-check and refinement)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gg2KCYOw-m_"
      },
      "source": [
        "The judge checks if the answer is grounded in the context.\n",
        "If not, it suggests a revised answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "DVSIfSmAwvvp"
      },
      "outputs": [],
      "source": [
        "def judge_agent(question: str, answer: str, research_output: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Judge checks grounding, coherence, and suggests improvement.\n",
        "    Returns dict with:\n",
        "      - verdict: \"OK\" or \"REVISE\"\n",
        "      - comment: explanation\n",
        "      - improved_answer: optional revised answer\n",
        "    \"\"\"\n",
        "    context = build_context_snippet(research_output)\n",
        "    system = \"You are a strict but fair evaluator of RAG answers.\"\n",
        "    user = f\"\"\"\n",
        "You evaluate an answer produced by a RAG system.\n",
        "\n",
        "QUESTION:\n",
        "{question}\n",
        "\n",
        "CONTEXT (from KBs and tools):\n",
        "{context}\n",
        "\n",
        "ANSWER TO EVALUATE:\n",
        "{answer}\n",
        "\n",
        "TASK:\n",
        "1. Decide if the answer is well grounded in the context and factually consistent.\n",
        "2. If it is mostly correct and grounded, set \"verdict\" to \"OK\".\n",
        "3. If it misses key information, hallucinates, or contradicts the context, set \"verdict\" to \"REVISE\".\n",
        "4. If verdict is \"REVISE\", also provide a corrected answer in \"improved_answer\".\n",
        "5. Output ONLY JSON with keys: \"verdict\", \"comment\", \"improved_answer\".\n",
        "\n",
        "Example:\n",
        "{{\"verdict\": \"OK\", \"comment\": \"Well grounded.\", \"improved_answer\": \"\"}}\n",
        "\"\"\".strip()\n",
        "\n",
        "    raw = call_llm(system, user)\n",
        "    try:\n",
        "        j = json.loads(raw)\n",
        "    except json.JSONDecodeError:\n",
        "        m = re.search(r\"\\{.*\\}\", raw, re.DOTALL)\n",
        "        if m:\n",
        "            j = json.loads(m.group(0))\n",
        "        else:\n",
        "            j = {\"verdict\": \"OK\", \"comment\": \"Failed to parse judge output.\", \"improved_answer\": \"\"}\n",
        "    return j\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avEJpLv-xEbF"
      },
      "source": [
        "## Agentic RAG controller (full pipeline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlFZJlWdxPEF"
      },
      "source": [
        "All the flow together:\n",
        "\n",
        "1. Plan\n",
        "\n",
        "2. Research\n",
        "\n",
        "3. Initial answer\n",
        "\n",
        "4. Judge + possible revision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "0UbrPAJ_xASu"
      },
      "outputs": [],
      "source": [
        "def agentic_rag(question: str) -> dict:\n",
        "    plan = plan_agent(question)\n",
        "\n",
        "    research_out = research_agent(question, plan)\n",
        "\n",
        "    answer1 = answer_agent(research_out)\n",
        "\n",
        "    judgment = judge_agent(question, answer1, research_out)\n",
        "\n",
        "    final_answer = answer1\n",
        "    if judgment.get(\"verdict\") == \"REVISE\" and judgment.get(\"improved_answer\"):\n",
        "        final_answer = judgment[\"improved_answer\"]\n",
        "\n",
        "    return {\n",
        "        \"question\": question,\n",
        "        \"plan\": plan,\n",
        "        \"research\": research_out,\n",
        "        \"initial_answer\": answer1,\n",
        "        \"judgment\": judgment,\n",
        "        \"final_answer\": final_answer,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSfiMmThxykK"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "aM-60Q7Bx11k"
      },
      "outputs": [],
      "source": [
        "test_questions = [\n",
        "    \"Explain how the Hamiltonian cycle in a graph can be found.\",\n",
        "    \"What are some topics in the AG News dataset about the stock market or oil?\",\n",
        "    \"3*(7+5) - 4\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGmTCTEnxZS2",
        "outputId": "c4f117aa-077a-4a07-ba07-1a2a0bef6677"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================\n",
            "QUESTION: Explain how the Hamiltonian cycle in a graph can be found.\n",
            "PLAN: {'mode': 'RAG', 'sources': ['squad'], 'use_calculator': False}\n",
            "\n",
            "INITIAL ANSWER:\n",
            " I'm not sure. The context provided does not contain information about how to find a Hamiltonian cycle in a graph. \n",
            "\n",
            "Sources: KB squad\n",
            "\n",
            "JUDGMENT: {'verdict': 'REVISE', 'comment': 'The answer correctly identifies that the context does not provide information about Hamiltonian cycles, but it does not attempt to explain how to find a Hamiltonian cycle based on common knowledge. Therefore, the answer lacks completeness.', 'improved_answer': 'To find a Hamiltonian cycle in a graph, one can use several approaches, including backtracking algorithms, dynamic programming, or heuristic methods such as genetic algorithms. The backtracking approach involves exploring all possible paths in the graph, attempting to construct the cycle by adding one vertex at a time and checking if it leads to a complete cycle. Dynamic programming can be employed to solve the problem more efficiently by storing subproblem solutions. However, determining if a Hamiltonian cycle exists is known to be NP-complete, meaning that there is no known polynomial-time algorithm to solve all instances of the problem.'}\n",
            "\n",
            "FINAL ANSWER:\n",
            " To find a Hamiltonian cycle in a graph, one can use several approaches, including backtracking algorithms, dynamic programming, or heuristic methods such as genetic algorithms. The backtracking approach involves exploring all possible paths in the graph, attempting to construct the cycle by adding one vertex at a time and checking if it leads to a complete cycle. Dynamic programming can be employed to solve the problem more efficiently by storing subproblem solutions. However, determining if a Hamiltonian cycle exists is known to be NP-complete, meaning that there is no known polynomial-time algorithm to solve all instances of the problem.\n",
            "\n",
            "Used knowledge base: ['squad', 'squad', 'squad', 'squad']\n",
            "Tools: []\n",
            "======================================================\n",
            "QUESTION: What are some topics in the AG News dataset about the stock market or oil?\n",
            "PLAN: {'mode': 'RAG', 'sources': ['news'], 'use_calculator': False}\n",
            "\n",
            "INITIAL ANSWER:\n",
            " In the AG News dataset, some topics related to the stock market and oil include the impacts of soaring crude prices on stock market outlooks, as indicated in multiple news reports. For instance, concerns about the economy alongside rising oil prices are noted to potentially dampen stock market performance during challenging times such as the summer doldrums. Additionally, reports mention that skyrocketing oil prices can create economic challenges, particularly with impending political events like presidential elections. Furthermore, there are instances where the stock market experiences slight gains even while oil prices are high, demonstrating the interconnectedness of these factors. \n",
            "\n",
            "Sources: news\n",
            "\n",
            "JUDGMENT: {'verdict': 'OK', 'comment': 'Well grounded.', 'improved_answer': ''}\n",
            "\n",
            "FINAL ANSWER:\n",
            " In the AG News dataset, some topics related to the stock market and oil include the impacts of soaring crude prices on stock market outlooks, as indicated in multiple news reports. For instance, concerns about the economy alongside rising oil prices are noted to potentially dampen stock market performance during challenging times such as the summer doldrums. Additionally, reports mention that skyrocketing oil prices can create economic challenges, particularly with impending political events like presidential elections. Furthermore, there are instances where the stock market experiences slight gains even while oil prices are high, demonstrating the interconnectedness of these factors. \n",
            "\n",
            "Sources: news\n",
            "\n",
            "Used knowledge base: ['news', 'news', 'news', 'news']\n",
            "Tools: []\n",
            "======================================================\n",
            "QUESTION: 3*(7+5) - 4\n",
            "PLAN: {'mode': 'DIRECT', 'sources': [], 'use_calculator': True}\n",
            "\n",
            "INITIAL ANSWER:\n",
            " To solve the expression \\(3 \\times (7 + 5) - 4\\):\n",
            "\n",
            "1. First, calculate the expression in the parentheses:\n",
            "   \\(7 + 5 = 12\\).\n",
            "\n",
            "2. Next, multiply by 3:\n",
            "   \\(3 \\times 12 = 36\\).\n",
            "\n",
            "3. Finally, subtract 4:\n",
            "   \\(36 - 4 = 32\\).\n",
            "\n",
            "Thus, the final answer is \\(32\\).\n",
            "\n",
            "JUDGMENT: {'verdict': 'OK', 'comment': 'Well grounded and correct.', 'improved_answer': ''}\n",
            "\n",
            "FINAL ANSWER:\n",
            " To solve the expression \\(3 \\times (7 + 5) - 4\\):\n",
            "\n",
            "1. First, calculate the expression in the parentheses:\n",
            "   \\(7 + 5 = 12\\).\n",
            "\n",
            "2. Next, multiply by 3:\n",
            "   \\(3 \\times 12 = 36\\).\n",
            "\n",
            "3. Finally, subtract 4:\n",
            "   \\(36 - 4 = 32\\).\n",
            "\n",
            "Thus, the final answer is \\(32\\).\n",
            "\n",
            "Used knowledge base: []\n",
            "Tools: ['calculator']\n"
          ]
        }
      ],
      "source": [
        "for q in test_questions:\n",
        "    result = agentic_rag(q)\n",
        "    print(\"======================================================\")\n",
        "    print(\"QUESTION:\", q)\n",
        "    print(\"PLAN:\", result[\"plan\"])\n",
        "    print(\"\\nINITIAL ANSWER:\\n\", result[\"initial_answer\"])\n",
        "    print(\"\\nJUDGMENT:\", result[\"judgment\"])\n",
        "    print(\"\\nFINAL ANSWER:\\n\", result[\"final_answer\"])\n",
        "    print(\"\\nUsed knowledge base:\", [c[\"kb\"] for c in result[\"research\"][\"chunks\"]][:5])\n",
        "    print(\"Tools:\", [t[\"tool\"] for t in result[\"research\"][\"tools\"]])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "RdVxLYgpE9It",
        "bdmnUtP6pXZ2"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.14.0)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
